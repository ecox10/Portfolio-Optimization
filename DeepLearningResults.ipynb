{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff2863d-6722-4e8a-9d07-3fbea0afece4",
   "metadata": {},
   "source": [
    "# Complete Deep Learning Optimization\n",
    "\n",
    "Perform optimization accfrom tensorflow.keras.layers import LSTM, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from keras.callbacks import EarlyStoppingording to the following:\n",
    "\n",
    "| | | | \n",
    "|----------------------------------------------------------------------|-----------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| n=4 $returns \\sim \\mathcal{N}(\\mu,\\sigma)$ Correlation from 0.8-0.9  | n=4 $returns \\sim N(\\mu,\\sigma)$ Independent  | n=4 $returns \\sim \\mathcal{N}(\\mu, sigma)$ Negative correlation (-0.8 to -0.5)  |\n",
    "| n=8 $returns \\sim \\mathcal{N}(\\mu,\\sigma)$ Correlation from 0.8-0.9  | n=8 $returns \\sim N(\\mu,\\sigma)$ Independent  | n=8 $returns \\sim \\mathcal{N}(\\mu, sigma)$ Negative correlation (-0.8 to -0.5)  |\n",
    "| n=16 $returns \\sim \\mathcal{N}(\\mu,\\sigma)$ Correlation from 0.8-0.9 | n=16 $returns \\sim N(\\mu,\\sigma)$ Independent | n=16 $returns \\sim \\mathcal{N}(\\mu, sigma)$ Negative correlation (-0.8 to -0.5) |\n",
    "\n",
    "using the method laid out in 4.3, portfolio optimization as done in https://www.quantconnect.com/terminal/processCache/?request=embedded_backtest_4ebbe01bfea8c5ae6f98fcda38a50b1c.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "436ca263-c197-498e-814e-6382f4088d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec7bb1-c177-4c38-9053-88ebee6c8d25",
   "metadata": {},
   "source": [
    "## Set up the Model \n",
    "\n",
    "I *think* I will need to reinitialize this class with the correct \"data\" for each estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2bd27e78-db0f-471e-bfb2-a14e07d90a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the Data\n",
    "file_name = \"PortfolioOptimizationData.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "loaded_list = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name_os = \"PortfolioOptimizationData_Zeros.pkl\"\n",
    "\n",
    "open_file_os = open(file_name_os, \"rb\")\n",
    "loaded_list_os = pickle.load(open_file_os)\n",
    "open_file_os.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b53e284b-38bd-4c2c-86b1-8188ade08be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n4_corr = loaded_list[0]\n",
    "data_n8_corr = loaded_list[1]\n",
    "data_n16_corr = loaded_list[2]\n",
    "\n",
    "data_n4_ind = loaded_list[3]\n",
    "data_n8_ind = loaded_list[4]\n",
    "data_n16_ind = loaded_list[5]\n",
    "\n",
    "data_n4_neg = loaded_list[6]\n",
    "data_n8_neg = loaded_list[7]\n",
    "data_n16_neg = loaded_list[8]\n",
    "\n",
    "data_n4_corr_os = loaded_list_os[0]\n",
    "data_n8_corr_os = loaded_list_os[1]\n",
    "data_n16_corr_os = loaded_list_os[2]\n",
    "\n",
    "data_n4_ind_os = loaded_list_os[3]\n",
    "data_n8_ind_os = loaded_list_os[4]\n",
    "data_n16_ind_os = loaded_list_os[5]\n",
    "\n",
    "data_n4_neg_os = loaded_list_os[6]\n",
    "data_n8_neg_os = loaded_list_os[7]\n",
    "data_n16_neg_os = loaded_list_os[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "504d30e0-363f-4226-b17d-00d66de7bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily surpress output\n",
    "#sys.stdout = open(os.devnull, \"w\")\n",
    "#sys.stderr = open(os.devnull, \"w\")\n",
    "# to return it\n",
    "sys.stdout = sys.__stdout__\n",
    "sys.stderr = sys.__stderr__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f822c-3f2a-4d35-b261-abfc9f74ded6",
   "metadata": {},
   "source": [
    "## Perform the optimization for positively simulated data\n",
    "Correlation from 0.8-0.9\n",
    "Training Data for Positively correlated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c6ccff65-526e-4408-8433-6b7f7e60cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No zeros\n",
    "n_assets = 4\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "91236808-1332-4cfb-9d1a-4798ef645c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n4_corr = np.zeros((100,n_assets))\n",
    "return_n4_corr = np.zeros((100,n_assets))\n",
    "risks_n4_corr = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_corr[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n4_corr[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n4_corr[i,j] = np.sum(weight_n4_corr[i,j] * data_n4_corr[i][j])\n",
    "        risks_n4_corr[i,j] = weight_n4_corr[i,j] * np.diagonal(np.cov(data_n4_corr[i].T))[j] * weight_n4_corr[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "189a36ee-c3a0-46d0-8209-ec6ee8dedb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('weights (mean) = '+str(np.round(np.mean(weight_n4_corr, axis=0),3)),\n",
    "      '\\nreturns (mean) = '+str(np.round(np.mean(return_n4_corr, axis=0),3)),\n",
    "      '\\nrisks (mean) ='+str(np.round(np.mean(risks_n4_corr, axis=0),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17b353a2-0134-4ec7-8a3c-2fa03224f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2fd41338-7db6-4634-9f86-07946156e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n4_corr_os = np.zeros((100,n_assets))\n",
    "return_n4_corr_os = np.zeros((100,n_assets))\n",
    "risks_n4_corr_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_corr_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n4_corr_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n4_corr_os[i,j] = np.sum(weight_n4_corr_os[i,j] * data_n4_corr_os[i][j])\n",
    "        risks_n4_corr_os[i,j] = weight_n4_corr_os[i,j] * np.diagonal(np.cov(data_n4_corr_os[i].T))[j] * weight_n4_corr_os[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3624c29-6c74-47da-a0f6-75ae2f355b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 8 assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "47356be2-3dee-44ee-94a6-c9f01fcf9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 8\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ace799-4528-41c9-a759-d84d55551b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e2bdd5ce-6a71-4589-bdd1-b1ba93bbf166",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n8_corr = np.zeros((100,n_assets))\n",
    "return_n8_corr = np.zeros((100,n_assets))\n",
    "risks_n8_corr = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_corr[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n8_corr[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n8_corr[i,j] = np.sum(weight_n8_corr[i,j] * data_n8_corr[i][j])\n",
    "        risks_n8_corr[i,j] = weight_n8_corr[i,j] * np.diagonal(np.cov(data_n8_corr[i].T))[j] * weight_n8_corr[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8171c02-d15c-4490-b022-504c8c26c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "da8c4486-871b-4246-b8b8-af36723853c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n8_corr_os = np.zeros((100,n_assets))\n",
    "return_n8_corr_os = np.zeros((100,n_assets))\n",
    "risks_n8_corr_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n8_corr_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n8_corr_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n8_corr_os[i,j] = np.sum(weight_n8_corr_os[i,j] * data_n8_corr_os[i][j])\n",
    "        risks_n8_corr_os[i,j] = weight_n8_corr_os[i,j] * np.diagonal(np.cov(data_n8_corr_os[i].T))[j] * weight_n8_corr_os[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9296d7b2-078d-4c82-bee2-142d3171ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f1a1b49b-acfa-441a-b721-a55c2bf61d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 16\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "79d2b7e0-459e-4e69-b465-8fe513f40ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1000, 4)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no zeros\n",
    "data_n16_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f0d8dbbc-470e-43b5-8678-d6e12c9c071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n16_corr = np.zeros((100,n_assets))\n",
    "return_n16_corr = np.zeros((100,n_assets))\n",
    "risks_n16_corr = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n16_corr[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n16_corr[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n16_corr[i,j] = np.sum(weight_n16_corr[i,j] * data_n16_corr[i][j])\n",
    "        risks_n16_corr[i,j] = weight_n16_corr[i,j] * np.diagonal(np.cov(data_n16_corr[i].T))[j] * weight_n16_corr[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cb70e-a032-4926-a96a-e9549286ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "da8a7f7f-53e4-46b1-aa01-a32d76be67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n16_corr_os = np.zeros((100,n_assets))\n",
    "return_n16_corr_os = np.zeros((100,n_assets))\n",
    "risks_n16_corr_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n16_corr_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n16_corr_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n16_corr_os[i,j] = np.sum(weight_n16_corr_os[i,j] * data_n16_corr_os[i][j])\n",
    "        risks_n16_corr_os[i,j] = weight_n16_corr_os[i,j] * np.diagonal(np.cov(data_n16_corr_os[i].T))[j] * weight_n16_corr_os[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37bce4-9223-43ad-bd1d-55509aff6895",
   "metadata": {},
   "source": [
    "## Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f7db2-3854-4e22-b2e3-afcdf660d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4 assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ca501bbb-2d1f-4bce-93f5-911ec044bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 4\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56615c2-9a8d-487c-a5b0-c3dd69771876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b91b82fc-0e70-4a12-a33a-9bcadeff62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n4_ind = np.zeros((100,n_assets))\n",
    "return_n4_ind = np.zeros((100,n_assets))\n",
    "risks_n4_ind = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_ind[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n4_ind[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n4_ind[i,j] = np.sum(weight_n4_ind[i,j] * data_n4_ind[i][j])\n",
    "        risks_n4_ind[i,j] = weight_n4_ind[i,j] * np.diagonal(np.cov(data_n4_ind[i].T))[j] * weight_n4_ind[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324f92e-cfe3-469b-a43c-666ebd996d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c7329d48-fc7a-4e80-adc1-f878cee9acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n4_ind_os = np.zeros((100,n_assets))\n",
    "return_n4_ind_os = np.zeros((100,n_assets))\n",
    "risks_n4_ind_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_ind_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n4_ind_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n4_ind_os[i,j] = np.sum(weight_n4_ind_os[i,j] * data_n4_ind_os[i][j])\n",
    "        risks_n4_ind_os[i,j] = weight_n4_ind_os[i,j] * np.diagonal(np.cov(data_n4_ind_os[i].T))[j] * weight_n4_ind_os[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2f05e-001a-436f-b651-c15df102af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c4be2120-1a40-4be3-86a9-ffadc94ad825",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 8\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0f06f-5977-4858-9eb2-1a3fd4b83091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2d4621b9-a2ac-4223-b74e-c96a467e28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n8_ind = np.zeros((100,n_assets))\n",
    "return_n8_ind = np.zeros((100,n_assets))\n",
    "risks_n8_ind = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n8_ind[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n8_ind[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n8_ind[i,j] = np.sum(weight_n8_ind[i,j] * data_n8_ind[i][j])\n",
    "        risks_n8_ind[i,j] = weight_n8_ind[i,j] * np.diagonal(np.cov(data_n8_ind[i].T))[j] * weight_n8_ind[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95129d3-f224-4f94-99cf-4241e9d2887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "113a5fd6-b5fd-4d37-b1ca-7490e001ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n8_ind_os = np.zeros((100,n_assets))\n",
    "return_n8_ind_os = np.zeros((100,n_assets))\n",
    "risks_n8_ind_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n8_ind_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n8_ind_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n8_ind_os[i,j] = np.sum(weight_n8_ind_os[i,j] * data_n8_ind_os[i][j])\n",
    "        risks_n8_ind_os[i,j] = weight_n8_ind_os[i,j] * np.diagonal(np.cov(data_n8_ind_os[i].T))[j] * weight_n8_ind_os[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b87d86-4180-49dd-976e-508ab0b7abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1c79d1b5-16c8-4997-9698-d0097ba7e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 16\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09415e-d827-4b4e-b740-77d9cb240109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f127b416-2998-4297-8a74-1d85ae235fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n16_ind = np.zeros((100,n_assets))\n",
    "return_n16_ind = np.zeros((100,n_assets))\n",
    "risks_n16_ind = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n16_ind[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n16_ind[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n16_ind[i,j] = np.sum(weight_n16_ind[i,j] * data_n16_ind[i][j])\n",
    "        risks_n16_ind[i,j] = weight_n16_ind[i,j] * np.diagonal(np.cov(data_n16_ind[i].T))[j] * weight_n16_ind[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f16a049d-99b8-4037-80a4-5b3a07ff2d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1000, 16)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c92177a7-c766-4e7d-b68d-806fa41d8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n16_ind_os = np.zeros((100,n_assets))\n",
    "return_n16_ind_os = np.zeros((100,n_assets))\n",
    "risks_n16_ind_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n16_ind_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n16_ind_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n16_ind_os[i,j] = np.sum(weight_n16_ind_os[i,j] * data_n16_ind_os[i][j])\n",
    "        risks_n16_ind_os[i,j] = weight_n16_ind_os[i,j] * np.diagonal(np.cov(data_n16_ind_os[i].T))[j] * weight_n16_ind_os[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62d00a-03e4-4f92-9ac0-6144f76ddff6",
   "metadata": {},
   "source": [
    "## Negative Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd1f91-39ff-41a5-b2d0-809387669463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cb77f27d-142b-4705-878d-4c5070101eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 4\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac92a4b-507e-429f-9d1a-ae6b1d5cd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1e5ccc6f-8ad1-449c-af13-ffaa31cbc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n4_neg = np.zeros((100,n_assets))\n",
    "return_n4_neg = np.zeros((100,n_assets))\n",
    "risks_n4_neg = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_neg[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n4_neg[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n4_neg[i,j] = np.sum(weight_n4_neg[i,j] * data_n4_neg[i][j])\n",
    "        risks_n4_neg[i,j] = weight_n4_neg[i,j] * np.diagonal(np.cov(data_n4_neg[i].T))[j] * weight_n4_neg[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc64ce-7697-4162-a82e-96bc3f74b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "cd41da52-3a53-4f37-924b-f36648a9662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n4_neg_os = np.zeros((100,n_assets))\n",
    "return_n4_neg_os = np.zeros((100,n_assets))\n",
    "risks_n4_neg_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n4_neg_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n4_neg_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n4_neg_os[i,j] = np.sum(weight_n4_neg_os[i,j] * data_n4_neg_os[i][j])\n",
    "        risks_n4_neg_os[i,j] = weight_n4_neg_os[i,j] * np.diagonal(np.cov(data_n4_neg_os[i].T))[j] * weight_n4_neg_os[i,j]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad460b-ee97-41bd-b4d3-08bd00b6580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "94717dc4-dd3b-4e3c-a88d-bfc78553f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 8\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ba5b1592-25b5-4650-9a5c-695c1a5a7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n8_neg = np.zeros((100,n_assets))\n",
    "return_n8_neg = np.zeros((100,n_assets))\n",
    "risks_n8_neg = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n8_neg[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n8_neg[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n8_neg[i,j] = np.sum(weight_n8_neg[i,j] * data_n8_neg[i][j])\n",
    "        risks_n8_neg[i,j] = weight_n8_neg[i,j] * np.diagonal(np.cov(data_n8_neg[i].T))[j] * weight_n8_neg[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea257d-d97f-4c48-8193-fd72be68dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4fdc5a61-fcfe-4b9b-9fa8-a81014a26c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n8_neg_os = np.zeros((100,n_assets))\n",
    "return_n8_neg_os = np.zeros((100,n_assets))\n",
    "risks_n8_neg_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n8_neg_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n8_neg_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n8_neg_os[i,j] = np.sum(weight_n8_neg_os[i,j] * data_n8_neg_os[i][j])\n",
    "        risks_n8_neg_os[i,j] = weight_n8_neg_os[i,j] * np.diagonal(np.cov(data_n8_neg_os[i].T))[j] * weight_n8_neg_os[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340922fc-7fb1-449d-bcc9-e1cbee6cada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6b24190b-aced-4b8a-b970-4eebb1f7fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 16\n",
    "n_obs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd913e3-f233-4da6-b854-b72ba862eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1c7ca6b7-7ebc-4eb8-b9cd-f981707a1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n16_neg = np.zeros((100,n_assets))\n",
    "return_n16_neg = np.zeros((100,n_assets))\n",
    "risks_n16_neg = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n16_neg[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n16_neg[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n16_neg[i,j] = np.sum(weight_n16_neg[i,j] * data_n16_neg[i][j])\n",
    "        risks_n16_neg[i,j] = weight_n16_neg[i,j] * np.diagonal(np.cov(data_n16_neg[i].T))[j] * weight_n16_neg[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267fc06-a8f4-4216-95bf-5f3356be372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7448302b-2811-4836-b85e-d666aa208961",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_n16_neg_os = np.zeros((100,n_assets))\n",
    "return_n16_neg_os = np.zeros((100,n_assets))\n",
    "risks_n16_neg_os = np.zeros((100,n_assets))\n",
    "    \n",
    "for i in range(100):\n",
    "    data = data_n16_neg_os[i]\n",
    "    \n",
    "    class Model:\n",
    "        def __init__(self):\n",
    "            self.data = None\n",
    "            self.model = None\n",
    "        \n",
    "            # self.callback = EarlyStopping(monitor='loss', min_delta=.1, patience = 10)\n",
    "    \n",
    "        def __build_model(self, input_shape, outputs):\n",
    "            model = Sequential([\n",
    "                LSTM(64, input_shape=input_shape, activation='relu'),\n",
    "                Flatten(),\n",
    "                Dense(outputs, activation='softmax')\n",
    "            ])\n",
    "\n",
    "            def sharpe_loss(_, y_pred):\n",
    "                coeffs = tf.tile(y_pred, (self.data.shape[0], 1))\n",
    "            \n",
    "                portfolio_values = tf.reduce_sum(tf.multiply(coeffs, self.data), axis=1)\n",
    "            \n",
    "                portfolio_returns = (portfolio_values[1:] - portfolio_values[:-1]) / portfolio_values[:-1]  # % change formula\n",
    "\n",
    "                sharpe = K.mean(portfolio_returns) / K.std(portfolio_returns)\n",
    "            \n",
    "                # exp keeps relative ordering between positives and negatives\n",
    "                #   since we want to maximize sharp, while gradient descent minimizes the loss\n",
    "                #   we negate the Sharpe value\n",
    "                return K.exp(-sharpe)\n",
    "        \n",
    "            model.compile(loss=sharpe_loss, optimizer='adam')\n",
    "            return model\n",
    "    \n",
    "        def get_allocations(self, data):\n",
    "        \n",
    "        \n",
    "            # data with returns\n",
    "            data_w_ret = np.concatenate([ data.values[1:], data.pct_change().values[1:] ], axis=1)\n",
    "        \n",
    "            data = data.iloc[1:]\n",
    "            self.data = tf.cast(tf.constant(data), float)\n",
    "        \n",
    "            if self.model is None:\n",
    "                self.model = self.__build_model(data_w_ret.shape, len(data.columns))\n",
    "        \n",
    "            fit_predict_data = data_w_ret[np.newaxis,:]        \n",
    "            self.model.fit(fit_predict_data, np.zeros((1, len(data.columns))), epochs=10, shuffle=False)\n",
    "            return self.model.predict(fit_predict_data)[0]\n",
    "    \n",
    "    model = Model() \n",
    "    weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets)))\n",
    "    weight_n16_neg_os[i,:] = weights\n",
    "    for j in range(n_assets):\n",
    "        return_n16_neg_os[i,j] = np.sum(weight_n16_neg_os[i,j] * data_n16_neg_os[i][j])\n",
    "        risks_n16_neg_os[i,j] = weight_n16_neg_os[i,j] * np.diagonal(np.cov(data_n16_neg_os[i].T))[j] * weight_n16_neg_os[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74f2d0-5253-457a-b77b-415332830b45",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "5135f3d7-851a-4de7-9fb0-44210dc3d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepLearningResults_Weights = [weight_n4_corr, weight_n8_corr, weight_n16_corr, weight_n4_corr_os, \n",
    "                              weight_n8_corr_os, weight_n16_corr_os, weight_n4_ind, weight_n8_ind,\n",
    "                              weight_n16_ind, weight_n4_ind_os, weight_n8_ind_os, weight_n16_ind_os,\n",
    "                              weight_n4_neg, weight_n8_neg, weight_n16_neg, weight_n4_neg_os, \n",
    "                              weight_n8_neg_os, weight_n16_neg_os]\n",
    "\n",
    "file_name = \"DeepLearningResultsWeights.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(DeepLearningResults_Weights, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "6779029b-00b8-4aba-9bf2-4516e5a48060",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepLearningResults_Returns = [return_n4_corr, return_n8_corr, return_n16_corr, return_n4_corr_os, \n",
    "                              return_n8_corr_os, return_n16_corr_os, return_n4_ind, return_n8_ind,\n",
    "                              return_n16_ind, return_n4_ind_os, return_n8_ind_os, return_n16_ind_os,\n",
    "                              return_n4_neg, return_n8_neg, return_n16_neg, return_n4_neg_os, \n",
    "                              return_n8_neg_os, return_n16_neg_os]\n",
    "\n",
    "file_name = \"DeepLearningResultsReturns.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(DeepLearningResults_Returns, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "853a6f4c-1900-4efc-9c20-ebdc85a5bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepLearningResults_Risks = [risks_n4_corr, risks_n8_corr, risks_n16_corr, risks_n4_corr_os, \n",
    "                              risks_n8_corr_os, risks_n16_corr_os, risks_n4_ind, risks_n8_ind,\n",
    "                              risks_n16_ind, risks_n4_ind_os, risks_n8_ind_os, risks_n16_ind_os,\n",
    "                              risks_n4_neg, risks_n8_neg, risks_n16_neg, risks_n4_neg_os, \n",
    "                              risks_n8_neg_os, risks_n16_neg_os]\n",
    "\n",
    "file_name = \"DeepLearningResultsRisks.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(DeepLearningResults_Risks, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3ccb5-e52c-46c2-83bb-6843e5ab9814",
   "metadata": {},
   "source": [
    "## Archived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fe82e-36b9-494a-bf89-a1b0db8e5131",
   "metadata": {},
   "source": [
    "mae = np.zeros((99,1))\n",
    "weight_n4_corr = pd.DataFrame(weight_n4_corr).dropna()\n",
    "for i in range(99):\n",
    "    mae[i] = sklearn.metrics.mean_absolute_error(np.array(weight_n4_corr)[i,:], [0.25, 0.25,0.25,0.25])\n",
    "mae_5epoch = mse.mean()\n",
    "mae_5epoch\n",
    "np.array([mse_20epoch, mse_30epoch, mse_40epoch, mse_50epoch, mse_60epoch, mse_70epoch, mse_80epoch])\n",
    "np.array([mae_20epoch, mae_30epoch, mae_40epoch, mae_50epoch, mae_60epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a804867-13d3-498d-8215-1d5307ee32c1",
   "metadata": {},
   "source": [
    "def one_optimization(n_assets, n_obs, r):\n",
    "    '''\n",
    "    First, simulates portfolios then optimizes. \n",
    "    This does 100 replications\\\n",
    "    '''\n",
    "    weight_res = np.zeros((9,n_assets))\n",
    "    #return_res = np.zeros((99,n_assets))\n",
    "    #risks_res = np.zeros((99,n_assets))\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    for i in range(9):\n",
    "        model = Model() \n",
    "        weights = model.get_allocations(pd.DataFrame(np.random.randn(n_assets,n_assets))) # what is this doing? This is kind of immportant to the model stability\n",
    "        weight_res[i,:] = weights\n",
    "        \n",
    "        rng = np.random.default_rng()\n",
    "        return_vec = rng.multivariate_normal(np.zeros(n_assets), cov = r, size = n_obs)# gives nxk data \n",
    "        for j in range(n_assets):\n",
    "            ret[j] = np.sum(wt[j] * return_vec[:,j])\n",
    "            rsk[j] = wt[j] * np.diagonal(np.cov(returns_vec))[j] * wt[j]\n",
    "        return_res[i,:] = returns.reshape(n_assets,)\n",
    "        risks_res[i,:] = risks.reshape(n_assets,)\n",
    "    return weight_res, return_res, risks_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
